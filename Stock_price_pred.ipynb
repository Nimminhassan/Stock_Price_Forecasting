{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTvgov4-2EBW"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zmCfne2Hi2"
      },
      "outputs": [],
      "source": [
        "# Example: Download data for NTPC(NTPC.NS) for the past 10 year with 1-day intervals\n",
        "ticker = \"NTPC.NS\"\n",
        "data = yf.download(ticker, period=\"10y\", interval=\"1d\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = f\"{ticker.replace('.', '_')}_stock_data.csv\"\n",
        "data.to_csv(csv_filename)\n",
        "\n",
        "print(f\"Data saved to {csv_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcbIKOeU2buk"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(csv_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKC7wMlc639_"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muOCeUwx7Nsa"
      },
      "outputs": [],
      "source": [
        "data=data.drop([0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02gHCR9a7rWe"
      },
      "outputs": [],
      "source": [
        "data = data.reset_index(drop=True)   # Reset Index After Deleting Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1gXQMR973Rl"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8KkAEvD77QO"
      },
      "outputs": [],
      "source": [
        "# Rename the 'Price' column to 'Date'\n",
        "data.rename(columns={'Price': 'Date'}, inplace=True)\n",
        "\n",
        "# Show the updated column names\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SPDBD8_8Xiq"
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"cleaned_stock_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJylV_Ia9C9w"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEQTpzB-8r89"
      },
      "outputs": [],
      "source": [
        "cols_to_convert = data.columns.difference(['Date'])\n",
        "data[cols_to_convert] = data[cols_to_convert].apply(pd.to_numeric, errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO4iTdVw9R0N"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iRRtH-b-QV7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure 'Date' is datetime (in case not already done)\n",
        "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['Date'], data['Close'], color='blue', label='Close Price')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Stock Close Price Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L5rpzsz-gxI"
      },
      "outputs": [],
      "source": [
        "data.round(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GerPOD3T_B34"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Enrichment: Adding Technical Indicators**"
      ],
      "metadata": {
        "id": "MsGcjqbzhrOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas_ta\n",
        "# import pandas_ta as ta"
      ],
      "metadata": {
        "id": "4rwX2FjOhq7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # --- 1. Calculate Technical Indicators ---\n",
        "\n",
        "# # Simple Moving Averages (Trend Indicator)\n",
        "# data.ta.sma(length=20, append=True) # Adds a 'SMA_20' column\n",
        "# data.ta.sma(length=50, append=True) # Adds a 'SMA_50' column\n",
        "\n",
        "# # Relative Strength Index (Momentum Indicator)\n",
        "# data.ta.rsi(length=14, append=True) # Adds a 'RSI_14' column\n",
        "\n",
        "# # Moving Average Convergence Divergence (MACD - Momentum Indicator)\n",
        "# data.ta.macd(fast=12, slow=26, signal=9, append=True) # Adds MACD, MACDh, MACDs columns\n",
        "\n"
      ],
      "metadata": {
        "id": "D8UIgAmhhqwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "kFcBAeIHjTUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- 2. Select Relevant Features ---\n",
        "\n",
        "# # Drop original 'Open', 'High', 'Low', 'Adj Close' as 'Close' and the TIs capture this info.\n",
        "# # We keep 'Close' as the target and 'Volume' as a direct feature.\n",
        "# features_to_keep = ['Close', 'Volume', 'SMA_20', 'SMA_50', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9']\n",
        "# data1 = data[features_to_keep]\n",
        "\n"
      ],
      "metadata": {
        "id": "xTn84w9vhqjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data1"
      ],
      "metadata": {
        "id": "FEQoeJMkjwNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- 3. Handle NaN Values ---\n",
        "# # Indicators like SMA_50 and RSI need 50 days of data, creating NaNs at the start.\n",
        "# print(f\"Original rows: {len(data1)}\")\n",
        "# data1.dropna(inplace=True)\n",
        "# print(f\"Rows after dropping NaNs: {len(data1)} (Data is ready for scaling)\")"
      ],
      "metadata": {
        "id": "8LwHt-m5jwAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data1"
      ],
      "metadata": {
        "id": "6lf7Ilu9n9S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Assume 'data' is the DataFrame from the previous step (after dropping NaNs)\n",
        "# df_values = data1.values\n",
        "# target_column_index = 0 # Assuming 'Close' is the first column\n",
        "\n",
        "# df_values"
      ],
      "metadata": {
        "id": "XoxHnEIunW4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- 1. Split Data into Train and Test Sets ---\n",
        "# # Use a time-series split (e.g., first 80% for train, last 20% for test)\n",
        "# train_size = int(len(df_values) * 0.80)\n",
        "# train_data = df_values[:train_size]\n",
        "# test_data = df_values[train_size:]\n",
        "\n",
        "# train_size"
      ],
      "metadata": {
        "id": "ZrEisFu1nWJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- 2. Initialize and Fit Scaler on TRAIN DATA ONLY ---\n",
        "# # The scaler will be used to scale all features (i.e., all columns)\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# train_scaled = scaler.fit_transform(train_data)\n",
        "\n",
        "# print(train_scaled)\n",
        "\n",
        "# # --- 3. Transform Test Data ---\n",
        "# test_scaled = scaler.transform(test_data)"
      ],
      "metadata": {
        "id": "W48Zc1idnWAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # **IMPORTANT:** The 'scaler' object is now fitted and SAVED.\n",
        "# # You will pass this scaler object to your evaluation step later.\n",
        "\n",
        "# # --- 4. Sequence Creation ---\n",
        "# # (You already have this step, but here's the structure for completeness)\n",
        "# def create_sequences(data, lookback):\n",
        "#     X, Y = [], []\n",
        "#     for i in range(len(data) - lookback):\n",
        "#         X.append(data[i:(i + lookback), :]) # All features in the lookback window\n",
        "#         Y.append(data[i + lookback, target_column_index]) # Only the 'Close' price (index 0)\n",
        "#     return np.array(X), np.array(Y)\n",
        "\n",
        "# lookback = 60 # Set your desired lookback window (e.g., 60 days)\n",
        "# X_train, y_train = create_sequences(train_scaled, lookback)\n",
        "# X_test, y_test = create_sequences(test_scaled, lookback)\n",
        "\n",
        "# # X_train, y_train, X_test, y_test are now ready for model training."
      ],
      "metadata": {
        "id": "Kz1VgcR3nV3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- ASSUME:\n",
        "# # 1. 'scaler' is the fitted MinMaxScaler object saved in Step 2.1\n",
        "# # 2. 'predicted_scaled' is the output of model.predict(X_test).shape (N, 1)\n",
        "# # 3. 'y_test' is the actual scaled target (Close price).\n",
        "\n",
        "# # Step 1: Create a dummy array for predictions\n",
        "# # The scaler was fitted on ALL features, so we need a temporary array\n",
        "# # with the shape of the original data (N, number_of_features)\n",
        "\n",
        "# num_features = data.shape[1] # e.g., 8 features (Close, Volume, TIs...)\n",
        "# dummy_test_array = np.zeros((len(y_test), num_features))\n",
        "\n",
        "# # Step 2: Place the SCALED predictions into the first column (where 'Close' was)\n",
        "# dummy_test_array[:, target_column_index] = predicted_scaled.flatten()\n",
        "\n",
        "# # Step 3: INVERT the predictions back to the original price scale\n",
        "# predicted_price = scaler.inverse_transform(dummy_test_array)[:, target_column_index]\n",
        "\n",
        "# # Step 4: INVERT the actual values for correct comparison\n",
        "# # This requires a similar process for y_test\n",
        "# dummy_actual_array = np.zeros((len(y_test), num_features))\n",
        "# dummy_actual_array[:, target_column_index] = y_test.flatten()\n",
        "# actual_price = scaler.inverse_transform(dummy_actual_array)[:, target_column_index]\n",
        "\n",
        "# # Step 5: Calculate Metrics on UN-SCALED Data\n",
        "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# import math\n",
        "\n",
        "# mse = mean_squared_error(actual_price, predicted_price)\n",
        "# rmse = math.sqrt(mse)\n",
        "# mae = mean_absolute_error(actual_price, predicted_price)\n",
        "\n",
        "# print(f\"MAE (Unscaled): {mae:.4f}\")\n",
        "# print(f\"RMSE (Unscaled): {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "6jc83R1EnVrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlNy9LFynv3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IneNxEegnvwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvjYM-1Tnvjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cwvtFtjtnvgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EVCa90DZhqM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPeq5Ne_M24g"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTeReRPR_FHm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chsBVQ43KlRG"
      },
      "outputs": [],
      "source": [
        "# Use the 'Close' price only\n",
        "close_data = data[['Close']].dropna()\n",
        "\n",
        "# Scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "scaled_close = scaler.fit_transform(close_data)\n",
        "\n",
        "# Create sequences (e.g., use last 60 days to predict the next one)\n",
        "X, y = [], []\n",
        "window_size = 60\n",
        "\n",
        "for i in range(window_size, len(scaled_close)):\n",
        "    X.append(scaled_close[i - window_size:i])\n",
        "    y.append(scaled_close[i])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Reshape for LSTM [samples, timesteps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h636XcKVL9C4"
      },
      "outputs": [],
      "source": [
        "train_size = int(len(X) * 0.8)\n",
        "print(train_size)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvaVrGVcsh6R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class SAMOptimizer(tf.keras.optimizers.Optimizer):\n",
        "    def __init__(self, base_optimizer, rho=0.05, name=\"SAM\", **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._optimizer = base_optimizer\n",
        "        self.rho = rho\n",
        "\n",
        "    @tf.function\n",
        "    def _compute_gradients(self, loss, var_list, tape):\n",
        "        return tape.gradient(loss, var_list)\n",
        "\n",
        "    def apply_gradients(self, grads_and_vars, **kwargs):\n",
        "        return self._optimizer.apply_gradients(grads_and_vars, **kwargs)\n",
        "\n",
        "    def first_step(self, gradients, variables):\n",
        "        grad_norm = tf.linalg.global_norm(gradients)\n",
        "        scale = self.rho / (grad_norm + 1e-12)\n",
        "\n",
        "        # perturb weights\n",
        "        self.e_ws = []\n",
        "        for v, g in zip(variables, gradients):\n",
        "            e_w = g * scale\n",
        "            v.assign_add(e_w)\n",
        "            self.e_ws.append(e_w)\n",
        "\n",
        "    def second_step(self, gradients, variables):\n",
        "        # restore weights\n",
        "        for v, e_w in zip(variables, self.e_ws):\n",
        "            v.assign_sub(e_w)\n",
        "\n",
        "        # apply gradients from second step\n",
        "        self.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "def train_with_sam(model, optimizer, x, y):\n",
        "    # Custom training step implementing SAM\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x, training=True)\n",
        "        loss = model.compiled_loss(y, y_pred)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Step 1: move to perturbed weights\n",
        "    optimizer.first_step(gradients, model.trainable_variables)\n",
        "\n",
        "    # Step 2: recompute loss at perturbed weights\n",
        "    with tf.GradientTape() as tape2:\n",
        "        y_pred2 = model(x, training=True)\n",
        "        loss2 = model.compiled_loss(y, y_pred2)\n",
        "    gradients2 = tape2.gradient(loss2, model.trainable_variables)\n",
        "    optimizer.second_step(gradients2, model.trainable_variables)\n",
        "    return loss, loss2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AOJn7dUshhR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pDqRYVBMD-Q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiBWTcoNK7oT"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qAIgKgdMGSS"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQD9k1_RMIoF"
      },
      "outputs": [],
      "source": [
        "predicted_scaled = model.predict(X_test)\n",
        "predicted = scaler.inverse_transform(predicted_scaled)\n",
        "actual = scaler.inverse_transform(y_test.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E7RZ55UMX95"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(actual, label='Actual Closing Price')\n",
        "plt.plot(predicted, label='Predicted Closing Price')\n",
        "plt.title('NTPC Stock Price Prediction using LSTM')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwCXCH8jMaap"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "\n",
        "mse = mean_squared_error(actual, predicted)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(actual, predicted)\n",
        "\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOymIuFpO7nM"
      },
      "source": [
        "**LSTM + DNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B0Yu2KpMeTI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "Model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "Model.add(LSTM(units=64, return_sequences=False, input_shape=(X_train.shape[1], 1)))\n",
        "Model.add(Dropout(0.2))\n",
        "\n",
        "# DNN layers (fully connected)\n",
        "Model.add(Dense(units=64, activation='relu'))\n",
        "Model.add(Dropout(0.2))\n",
        "Model.add(Dense(units=32, activation='relu'))\n",
        "Model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "Model.add(Dense(units=1))  # Predicting 1 value (stock price)\n",
        "\n",
        "# Compile the model\n",
        "Model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjMV4pJ9N6NU"
      },
      "outputs": [],
      "source": [
        "history = Model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxwkUKNkN_Pw"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "predicted_scaled = Model.predict(X_test)\n",
        "predicted = scaler.inverse_transform(predicted_scaled)\n",
        "actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(actual, label='Actual Price')\n",
        "plt.plot(predicted, label='Predicted Price')\n",
        "plt.title('NTPC Stock Price Prediction (LSTM + DNN)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfEm-T3WOM5d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "\n",
        "mse = mean_squared_error(actual, predicted)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(actual, predicted)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox_OGtilPJgb"
      },
      "source": [
        "**BILSTM + DNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5aXhmjvOhVV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, LSTM\n",
        "\n",
        "mod = Sequential()\n",
        "\n",
        "# BiLSTM Layer\n",
        "mod.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(X_train.shape[1], 1)))\n",
        "mod.add(Dropout(0.2))\n",
        "\n",
        "# DNN Layers\n",
        "mod.add(Dense(64, activation='relu'))\n",
        "mod.add(Dropout(0.2))\n",
        "mod.add(Dense(32, activation='relu'))\n",
        "mod.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer\n",
        "mod.add(Dense(1))  # Predict next day's price\n",
        "\n",
        "# Compile Model\n",
        "mod.compile(optimizer='adam', loss='mean_squared_error')\n",
        "mod.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atkfn5-lPpGV"
      },
      "outputs": [],
      "source": [
        "hist = mod.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ir-xIWLPwrg"
      },
      "outputs": [],
      "source": [
        "predicted_scaled = mod.predict(X_test)\n",
        "predicted = scaler.inverse_transform(predicted_scaled)\n",
        "actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MekZ0KuP2JV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(actual, label='Actual')\n",
        "plt.plot(predicted, label='Predicted')\n",
        "plt.title('NTPC Stock Price Prediction (BiLSTM + DNN)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Stock Price (INR)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq-8XrTFQJ8N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "\n",
        "mse = mean_squared_error(actual, predicted)\n",
        "rmse = math.sqrt(mse)\n",
        "mae = mean_absolute_error(actual, predicted)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvAnfETMQckl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}